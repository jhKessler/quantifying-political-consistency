{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "13459f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "from src import config\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "gpt = pd.read_parquet(\"output/predictions_gpt_4_1.parquet\")\n",
    "gptMini = pd.read_parquet(\"output/predictions_gpt_4_1_mini.parquet\")\n",
    "\n",
    "dc = pd.read_parquet(\"output/predictions_deepseek_chat.parquet\")\n",
    "dcR = pd.read_parquet(\"output/predictions_deepseek_reasoner.parquet\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "164d74ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5168\n",
      "F1 Score (Macro): 0.4254\n",
      "F1 Score (Weighted): 0.5606\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "y_true = dcR['ground_truth']\n",
    "y_pred = dcR['prediction']\n",
    "\n",
    "# --- Calculate Metrics ---\n",
    "\n",
    "# 1. Accuracy\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# For multiclass classification, you should specify the 'average' parameter.\n",
    "# 'macro' calculates the metric for each label, and finds their unweighted mean.\n",
    "f1_macro = f1_score(y_true, y_pred, average='macro')\n",
    "print(f\"F1 Score (Macro): {f1_macro:.4f}\")\n",
    "\n",
    "# 3. Weighted F1 Score\n",
    "# This calculates the metric for each label, and finds their average,\n",
    "# weighted by the number of true instances for each label (the support).\n",
    "f1_weighted = f1_score(y_true, y_pred, average='weighted')\n",
    "print(f\"F1 Score (Weighted): {f1_weighted:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2960b2d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "vote_correct\n",
       "False    0.972166\n",
       "True     0.986561\n",
       "Name: discipline, dtype: float64"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dc.groupby(\"vote_correct\")[\"discipline\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75086298",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t-Statistik: 6.3589\n",
      "p-Wert: 2.329985795012167e-10\n"
     ]
    }
   ],
   "source": [
    "from scipy import stats\n",
    "discipline_true = dc[dc[\"vote_correct\"] == True][\"discipline\"]\n",
    "discipline_false = dc[dc[\"vote_correct\"] == False][\"discipline\"]\n",
    "t_statistic, p_value = stats.ttest_ind(discipline_true, discipline_false, equal_var=False)\n",
    "# --- 4. Ergebnisse ausgeben --- \n",
    "print(f\"t-Statistik: {t_statistic:.4f}\")\n",
    "print(f\"p-Wert: {p_value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1fe8edcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Manual T-statistic: 6.358891035688536\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "x_mean = discipline_true.mean()\n",
    "x_std = discipline_true.std()\n",
    "y_mean = discipline_false.mean()\n",
    "y_std = discipline_false.std()\n",
    "\n",
    "# Get the number of samples in each group\n",
    "n_x = len(discipline_true)\n",
    "n_y = len(discipline_false)\n",
    "\n",
    "# Calculate the t-statistic manually using the formula for Welch's t-test\n",
    "numerator = x_mean - y_mean\n",
    "denominator = math.sqrt((x_std**2 / n_x) + (y_std**2 / n_y))\n",
    "t_statistic_manual = numerator / denominator\n",
    "\n",
    "print(f\"Manual T-statistic: {t_statistic_manual}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4db5a7b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.05654076642623574)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "x_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6fb972e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welch's Degrees of Freedom: 3108.4294460421056\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "np.float64(1.1567203789548698e-10)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This is the formula for the degrees of freedom when variances are not assumed to be equal\n",
    "s1_squared = x_std**2\n",
    "s2_squared = y_std**2\n",
    "\n",
    "numerator_df = ((s1_squared / n_x) + (s2_squared / n_y))**2\n",
    "denominator_df = ( (s1_squared / n_x)**2 / (n_x - 1) ) + ( (s2_squared / n_y)**2 / (n_y - 1) )\n",
    "\n",
    "welch_df = numerator_df / denominator_df\n",
    "print(f\"Welch's Degrees of Freedom: {welch_df}\")\n",
    "\n",
    "# --- Step 2: Calculate the one-tailed p-value ---\n",
    "# We use the survival function (sf), which is 1 - cdf.\n",
    "# This gives the area in the right tail of the distribution, which is\n",
    "# exactly what we want for the hypothesis that x_mean > y_mean.\n",
    "p_value_one_tailed = stats.t.sf(abs(6.36), df=welch_df)\n",
    "\n",
    "p_value_one_tailed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3386adff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(3108.4294460421056)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "welch_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fd88c1c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.9865608406243725)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "279182d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3460"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(gpt)-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48f5a480",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
