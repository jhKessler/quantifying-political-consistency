{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8dfdf30e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{table}[h!]\n",
      "\\centering\n",
      "\\begin{tabular}{l|ccccccc}\n",
      "\\hline\n",
      "Model & Accuracy & Macro P & Macro R & Macro F1 & Weighted P & Weighted R & Weighted F1 \\\\ \n",
      "\\hline\n",
      "GPT 4.1-mini & 0.577 & 0.493 & 0.455 & 0.438 & 0.676 & 0.577 & 0.580 \\\\ \n",
      "GPT 4.1 & 0.505 & 0.539 & 0.467 & 0.427 & 0.741 & 0.505 & 0.553 \\\\ \n",
      "Deepseek Chat & 0.415 & 0.520 & 0.452 & 0.384 & 0.713 & 0.415 & 0.490 \\\\ \n",
      "Deepseek Reasoner & 0.517 & 0.497 & 0.444 & 0.425 & 0.684 & 0.517 & 0.561 \\\\ \n",
      "\\hline\n",
      "\\end{tabular}\n",
      "\\caption{Model Performance Metrics (P: Precision, R: Recall)}\n",
      "\\label{tab:model_metrics}\n",
      "\\end{table}\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def calculate_metrics(cm, model_name):\n",
    "    \"\"\"Calculates and prints classification metrics from a confusion matrix.\"\"\"\n",
    "    n_classes = cm.shape[0]\n",
    "    metrics = {}\n",
    "\n",
    "    # Total instances\n",
    "    total_samples = np.sum(cm)\n",
    "\n",
    "    # Accuracy\n",
    "    accuracy = np.trace(cm) / total_samples\n",
    "    metrics['Accuracy'] = accuracy\n",
    "\n",
    "    # Per-class metrics\n",
    "    precision = []\n",
    "    recall = []\n",
    "    f1_score = []\n",
    "    true_positives = np.diag(cm)\n",
    "    false_positives = np.sum(cm, axis=0) - true_positives\n",
    "    false_negatives = np.sum(cm, axis=1) - true_positives\n",
    "\n",
    "    for i in range(n_classes):\n",
    "        p = true_positives[i] / (true_positives[i] + false_positives[i]) if (true_positives[i] + false_positives[i]) > 0 else 0\n",
    "        r = true_positives[i] / (true_positives[i] + false_negatives[i]) if (true_positives[i] + false_negatives[i]) > 0 else 0\n",
    "        f1 = 2 * (p * r) / (p + r) if (p + r) > 0 else 0\n",
    "        precision.append(p)\n",
    "        recall.append(r)\n",
    "        f1_score.append(f1)\n",
    "\n",
    "    # Macro averages\n",
    "    metrics['Macro Precision'] = np.mean(precision)\n",
    "    metrics['Macro Recall'] = np.mean(recall)\n",
    "    metrics['Macro F1-score'] = np.mean(f1_score)\n",
    "\n",
    "    # Weighted averages\n",
    "    samples_per_class = np.sum(cm, axis=1)\n",
    "    metrics['Weighted Precision'] = np.sum([p * s for p, s in zip(precision, samples_per_class)]) / total_samples\n",
    "    metrics['Weighted Recall'] = np.sum([r * s for r, s in zip(recall, samples_per_class)]) / total_samples\n",
    "    metrics['Weighted F1-score'] = np.sum([f1 * s for f1, s in zip(f1_score, samples_per_class)]) / total_samples\n",
    "\n",
    "    return {model_name: metrics}\n",
    "\n",
    "# --- Data from your request ---\n",
    "# Confusion matrices\n",
    "cm_gpt_4_1_mini = np.array([[708, 788, 221], [110, 1262, 170], [24, 152, 27]])\n",
    "cm_gpt_4_1 = np.array([[602, 497, 618], [48, 1073, 421], [5, 126, 72]])\n",
    "cm_deepseek_chat = np.array([[564, 366, 787], [77, 764, 701], [12, 83, 108]])\n",
    "cm_deepseek_reasoner = np.array([[683, 514, 520], [131, 1055, 356], [24, 128, 51]])\n",
    "\n",
    "# --- Calculations ---\n",
    "# Calculate metrics for all models\n",
    "results = {}\n",
    "results.update(calculate_metrics(cm_gpt_4_1_mini, 'GPT 4.1-mini'))\n",
    "results.update(calculate_metrics(cm_gpt_4_1, 'GPT 4.1'))\n",
    "results.update(calculate_metrics(cm_deepseek_chat, 'Deepseek Chat'))\n",
    "results.update(calculate_metrics(cm_deepseek_reasoner, 'Deepseek Reasoner'))\n",
    "\n",
    "\n",
    "# --- Output Generation ---\n",
    "# Generate LaTeX table\n",
    "latex_table = \"\\\\begin{table}[h!]\\n\\\\centering\\n\"\n",
    "latex_table += \"\\\\begin{tabular}{l|ccccccc}\\n\"\n",
    "latex_table += \"\\\\hline\\n\"\n",
    "latex_table += \"Model & Accuracy & Macro P & Macro R & Macro F1 & Weighted P & Weighted R & Weighted F1 \\\\\\\\ \\n\"\n",
    "latex_table += \"\\\\hline\\n\"\n",
    "\n",
    "for model, metrics in results.items():\n",
    "    latex_table += f\"{model} & \"\n",
    "    latex_table += f\"{metrics['Accuracy']:.3f} & \"\n",
    "    latex_table += f\"{metrics['Macro Precision']:.3f} & \"\n",
    "    latex_table += f\"{metrics['Macro Recall']:.3f} & \"\n",
    "    latex_table += f\"{metrics['Macro F1-score']:.3f} & \"\n",
    "    latex_table += f\"{metrics['Weighted Precision']:.3f} & \"\n",
    "    latex_table += f\"{metrics['Weighted Recall']:.3f} & \"\n",
    "    latex_table += f\"{metrics['Weighted F1-score']:.3f} \\\\\\\\ \\n\"\n",
    "\n",
    "latex_table += \"\\\\hline\\n\"\n",
    "latex_table += \"\\\\end{tabular}\\n\"\n",
    "latex_table += \"\\\\caption{Model Performance Metrics (P: Precision, R: Recall)}\\n\"\n",
    "latex_table += \"\\\\label{tab:model_metrics}\\n\"\n",
    "latex_table += \"\\\\end{table}\"\n",
    "\n",
    "print(latex_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f011fae6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
